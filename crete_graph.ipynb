{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_analyzer.config_load import load_config\n",
    "from pdf_analyzer.api import API\n",
    "from omegaconf import OmegaConf\n",
    "from pdf_analyzer.config_load import load_config\n",
    "\n",
    "from pdf_analyzer.api.extract.elements import extract_element_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_paper_metadata import get_all_paper_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVER_CONFIG\n",
      "url:\n",
      "  protocol: http\n",
      "  api_domain: yordi111nas.synology.me\n",
      "  port: 8070\n",
      "\n",
      "CLOUD_CONFIG\n",
      "data:\n",
      "  data_dir: data/PDFs\n",
      "  format: .pdf\n",
      "  recursive: true\n",
      "grobid:\n",
      "  cache: true\n",
      "  cache_dir: data/xmls\n",
      "  operation_key: processFulltextDocument\n",
      "  format: .grobid.tei.xml\n",
      "  recursive: true\n",
      "\n",
      "http://yordi111nas.synology.me:8070/api/isalive\n",
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-05-02 21:49:46,679 | INFO | API.py:44 | 4608 >>> All files have been process by the api\n"
     ]
    }
   ],
   "source": [
    "server_config = load_config(\"config/api/grobid-server-config.yaml\")\n",
    "extract_config = load_config(\"config/api/api-base-config.yaml\")\n",
    "print(\"SERVER_CONFIG\\n\"+OmegaConf.to_yaml(server_config))\n",
    "print(\"CLOUD_CONFIG\\n\"+OmegaConf.to_yaml(extract_config))\n",
    "\n",
    "base_api = API.BaseAPI(extract_config,server_config)\n",
    "\n",
    "files = base_api.proccesed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://open_science.com#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<http://open_science.com#Paper/0> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Exploring teachers’ confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2019-05-24\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Exploring teachers’ confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/1> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\n",
      "\"\"\" ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/2> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2020-05-05\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/3> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Pronunciation and good language learners</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2020-07-22\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Pronunciation and good language learners</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/4> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">\\\"Why Should I Trust You?\\\"</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2016-08-13\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">\\\"Why Should I Trust You?\\\"</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/5> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2021-10-16\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/6> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">A Robustly Optimized BERT Pre-training Approach with Post-training</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   \n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2019-07-26\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">A Robustly Optimized BERT Pre-training Approach with Post-training</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/7> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Preface to the book Fast Processes in Large Scale Atmospheric Models: Progress, Challenges and Opportunities</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2024-02-28\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Preface to the book Fast Processes in Large Scale Atmospheric Models: Progress, Challenges and Opportunities</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/8> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Attention Is All You Need</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.† Work performed while at Google Brain. ‡ Work performed while at Google Research.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2023-08-02\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Attention Is All You Need</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/9> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Efficient Estimation of Word Representations in Vector Space</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2013-09-07\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Efficient Estimation of Word Representations in Vector Space</title>\" .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Definir los prefijos de los namespaces\n",
    "mod = Namespace(\"https://w3id.org/mod#\")\n",
    "ns = Namespace(\"http://open_science.com#\")\n",
    "\n",
    "# Crear un grafo RDF\n",
    "g = Graph()\n",
    "\n",
    "# Función para convertir la fecha de formato XML a XSD\n",
    "def convert_date(xml_date):\n",
    "    formats = [\"%d %b %Y\", \"%Y-%m-%d\"]  # Probamos dos formatos diferentes\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(xml_date.text.strip(), fmt).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# Iterar sobre los datos de los papers y agregarlos al grafo\n",
    "papers_data =get_all_paper_metadata(files)\n",
    "for paper in papers_data:\n",
    "    paper_uri = URIRef(f\"http://open_science.com#Paper/{paper['paper_id']}\")\n",
    "    g.add((paper_uri, RDF.type, ns.Paper))\n",
    "    g.add((paper_uri, ns.title, Literal(paper['title'])))\n",
    "    g.add((paper_uri, ns.abstract, Literal(paper['abstract'])))\n",
    "    g.add((paper_uri, RDFS.label, Literal(paper['title'])))\n",
    "\n",
    "    # Utilizar BeautifulSoup para analizar el elemento XML de fecha\n",
    "    if paper['published_date']!=None:\n",
    "        xml_date = paper['published_date']\n",
    "        published_date = convert_date(xml_date)\n",
    "        if published_date:\n",
    "            g.add((paper_uri, ns.release_date, Literal(published_date, datatype=XSD.date)))\n",
    "\n",
    "    # Aquí puedes agregar más relaciones según sea necesario, como autores, organizaciones, etc.\n",
    "\n",
    "# Serializar el grafo RDF en formato Turtle\n",
    "print(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.extract_ner import get_all_ners,get_all_projects\n",
    "from extract_authors import get_all_author_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orgs_rel,all_orgs=get_all_ners(files,pipe)\n",
    "authors_list,relation_author_paper,all_orgs,relation_author_org =get_all_author_metadata(files,all_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for org in all_orgs:\n",
    "    org_uri = ns[f\"Organization/{org['org_id']}\"]\n",
    "    g.add((org_uri, RDF.type, ns.Organization))\n",
    "    g.add((org_uri, ns.name, Literal(org['name'])))\n",
    "    g.add((org_uri, RDFS.label, Literal(org['name'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in all_orgs_rel:\n",
    "    # URI de la organización\n",
    "    organization_uri = ns[f\"Organization/{dic['org_id']}\"]\n",
    "    \n",
    "    # URI del papel\n",
    "    paper_uri = ns[f\"Paper/{dic['paper_id']}\"]\n",
    "    \n",
    "    # Agregar tripleta al grafo RDF\n",
    "    g.add((paper_uri, ns.acknowledges, organization_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in relation_author_org:\n",
    "    # URI de la organización\n",
    "    organization_uri = ns[f\"Organization/{dic['org_id']}\"]\n",
    "    \n",
    "    # URI del papel\n",
    "    author_id = ns[f\"Author/{dic['author_id']}\"]\n",
    "    \n",
    "    # Agregar tripleta al grafo RDF\n",
    "    g.add((author_id, ns.member, organization_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in relation_author_org:\n",
    "    # URI de la organización\n",
    "    paper_uri = ns[f\"Paper/{dic['paper_id']}\"]\n",
    "    \n",
    "    # URI del papel\n",
    "    author_id = ns[f\"Author/{dic['author_id']}\"]\n",
    "    \n",
    "    # Agregar tripleta al grafo RDF\n",
    "    g.add((author_id, ns.author, paper_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_projects,all_projects_relation=get_all_projects(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in all_projects:\n",
    "\n",
    "    project_uri = ns[f\"Project/{project['project_id']}\"]\n",
    "\n",
    "    # Crear la etiqueta del proyecto\n",
    "    label = f\"Award {project['project_name']} {project['project_federal_id']}\"\n",
    "\n",
    "    # Agregar triples al grafo RDF\n",
    "    g.add((project_uri, RDF.type, ns.Project))\n",
    "    g.add((project_uri, ns.name, Literal(project['project_name'], datatype=XSD.string)))\n",
    "    g.add((project_uri, RDFS.label, Literal(label, datatype=XSD.string)))\n",
    "    g.add((project_uri, ns.project_federal_id, Literal(project['project_federal_id'], datatype=XSD.string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in all_projects_relation:\n",
    "    # URI del proyecto\n",
    "    project_uri = ns[f\"Project/{relation['project_id']}\"]\n",
    "    \n",
    "    # URI del paper\n",
    "    paper_uri = ns[f\"Paper/{relation['paper_id']}\"]\n",
    "    \n",
    "    # Agregar tripleta al grafo RDF\n",
    "    g.add((paper_uri, ns.acknowledges, project_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://open_science.com#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<http://open_science.com#Paper/0> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Exploring teachers’ confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2019-05-24\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Exploring teachers’ confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/1> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\n",
      "\"\"\" ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/2> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.\n",
      "\"\"\" ;\n",
      "    ns1:acknowledges <http://open_science.com#Organization/0>,\n",
      "        <http://open_science.com#Organization/1>,\n",
      "        <http://open_science.com#Organization/2>,\n",
      "        <http://open_science.com#Project/0> ;\n",
      "    ns1:release_date \"2020-05-05\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/3> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Pronunciation and good language learners</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n",
      "\"\"\" ;\n",
      "    ns1:acknowledges <http://open_science.com#Organization/3>,\n",
      "        <http://open_science.com#Organization/4> ;\n",
      "    ns1:release_date \"2020-07-22\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Pronunciation and good language learners</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/4> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">\\\"Why Should I Trust You?\\\"</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.\n",
      "\"\"\" ;\n",
      "    ns1:acknowledges <http://open_science.com#Organization/5>,\n",
      "        <http://open_science.com#Organization/6>,\n",
      "        <http://open_science.com#Organization/7>,\n",
      "        <http://open_science.com#Organization/8>,\n",
      "        <http://open_science.com#Organization/9>,\n",
      "        <http://open_science.com#Project/1>,\n",
      "        <http://open_science.com#Project/2> ;\n",
      "    ns1:release_date \"2016-08-13\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">\\\"Why Should I Trust You?\\\"</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/5> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2021-10-16\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/6> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">A Robustly Optimized BERT Pre-training Approach with Post-training</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   \n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2019-07-26\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">A Robustly Optimized BERT Pre-training Approach with Post-training</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/7> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Preface to the book Fast Processes in Large Scale Atmospheric Models: Progress, Challenges and Opportunities</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2024-02-28\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Preface to the book Fast Processes in Large Scale Atmospheric Models: Progress, Challenges and Opportunities</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/8> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Attention Is All You Need</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.† Work performed while at Google Brain. ‡ Work performed while at Google Research.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2023-08-02\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Attention Is All You Need</title>\" .\n",
      "\n",
      "<http://open_science.com#Paper/9> a ns1:Paper ;\n",
      "    rdfs:label \"<title level=\\\"a\\\" type=\\\"main\\\">Efficient Estimation of Word Representations in Vector Space</title>\" ;\n",
      "    ns1:abstract \"\"\" \n",
      "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n",
      "\"\"\" ;\n",
      "    ns1:release_date \"2013-09-07\"^^xsd:date ;\n",
      "    ns1:title \"<title level=\\\"a\\\" type=\\\"main\\\">Efficient Estimation of Word Representations in Vector Space</title>\" .\n",
      "\n",
      "<http://open_science.com#Organization/0> a ns1:Organization ;\n",
      "    rdfs:label \"Al-lenNLP\" ;\n",
      "    ns1:name \"Al-lenNLP\" .\n",
      "\n",
      "<http://open_science.com#Organization/1> a ns1:Organization ;\n",
      "    rdfs:label \"Office of Naval Research\" ;\n",
      "    ns1:name \"Office of Naval Research\" .\n",
      "\n",
      "<http://open_science.com#Organization/2> a ns1:Organization ;\n",
      "    rdfs:label \"Google\" ;\n",
      "    ns1:name \"Google\" .\n",
      "\n",
      "<http://open_science.com#Organization/3> a ns1:Organization ;\n",
      "    rdfs:label \"OpenAI\" ;\n",
      "    ns1:name \"OpenAI\" .\n",
      "\n",
      "<http://open_science.com#Organization/4> a ns1:Organization ;\n",
      "    rdfs:label \"WebText\" ;\n",
      "    ns1:name \"WebText\" .\n",
      "\n",
      "<http://open_science.com#Organization/5> a ns1:Organization ;\n",
      "    rdfs:label \"TerraSwarm\" ;\n",
      "    ns1:name \"TerraSwarm\" .\n",
      "\n",
      "<http://open_science.com#Organization/6> a ns1:Organization ;\n",
      "    rdfs:label \"STARnet\" ;\n",
      "    ns1:name \"STARnet\" .\n",
      "\n",
      "<http://open_science.com#Organization/7> a ns1:Organization ;\n",
      "    rdfs:label \"Semiconductor Research Corporation\" ;\n",
      "    ns1:name \"Semiconductor Research Corporation\" .\n",
      "\n",
      "<http://open_science.com#Organization/8> a ns1:Organization ;\n",
      "    rdfs:label \"MARCO\" ;\n",
      "    ns1:name \"MARCO\" .\n",
      "\n",
      "<http://open_science.com#Organization/9> a ns1:Organization ;\n",
      "    rdfs:label \"DARPA\" ;\n",
      "    ns1:name \"DARPA\" .\n",
      "\n",
      "<http://open_science.com#Project/0> a ns1:Project ;\n",
      "    rdfs:label \"Award MURI N00014-18-1-2670\"^^xsd:string ;\n",
      "    ns1:name \"MURI\"^^xsd:string ;\n",
      "    ns1:project_federal_id \"N00014-18-1-2670\"^^xsd:string .\n",
      "\n",
      "<http://open_science.com#Project/1> a ns1:Project ;\n",
      "    rdfs:label \"Award ONR #W911NF-13-1-0246\"^^xsd:string ;\n",
      "    ns1:name \"ONR\"^^xsd:string ;\n",
      "    ns1:project_federal_id \"#W911NF-13-1-0246\"^^xsd:string .\n",
      "\n",
      "<http://open_science.com#Project/2> a ns1:Project ;\n",
      "    rdfs:label \"Award ONR #N00014-13-1-0023\"^^xsd:string ;\n",
      "    ns1:name \"ONR\"^^xsd:string ;\n",
      "    ns1:project_federal_id \"#N00014-13-1-0023\"^^xsd:string .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
