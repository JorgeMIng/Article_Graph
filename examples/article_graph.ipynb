{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Graph Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a quick overview of the `article_graph` module together with the `topic_modeling`, `similarity` and `ner` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of mock papers to be used in this example\n",
    "\n",
    "papers = [\n",
    "    {\n",
    "        'title': 'Title 1',\n",
    "        'abstract': 'The universe is a vast expanse of space containing countless galaxies, stars, planets, and other celestial objects.',\n",
    "        'release_date': '2024-01-24',\n",
    "        'acknowledgements': '''\n",
    "            We thank the referees for their constructive feedback, which has helped us to improve the quality of this manuscript.\n",
    "            This work is based on spectropolarimetric observations obtained at the TBL, AATand 3.6-m ESO telescope.\n",
    "            We thank the technical staff at each of these facilities for their time and data.\n",
    "            We also acknowledge the use of the PolarBase database, which makes TBL observations publicly available,\n",
    "            and is operated by the Centre National de la Recherche Scientifique of France (CNRS), Observatoire''',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Title 2',\n",
    "        'abstract': 'Ancient civilizations such as the Egyptians, Greeks, and Romans have left behind rich legacies of art, architecture, and knowledge.',\n",
    "        'release_date': '2024-02-15',\n",
    "        'acknowledgements': '''\n",
    "            Acknowledgements. We are grateful to our referee, Nicolas Cowan.\n",
    "            We gratefully acknowledge the open source software which made this work possible:\n",
    "                astropy (Astropy Collaboration et al. 2013Collaboration et al. , 2018Collaboration et al. , 2022)),\n",
    "                ipython (Pérez &amp; Granger 2007),\n",
    "                numpy (Harris et al. 2020),\n",
    "                scipy (Virtanen et al. 2020),\n",
    "                matplotlib (Hunter 2007),\n",
    "                JAX (Bradbury et al. 2018),\n",
    "                arviz (Kumar et al. 2019),\n",
    "                numpyro (Phan et al. 2019),\n",
    "                FastChem (Stock et al. 2018(Stock et al. , 2022;;Kitzmann et al. 2023),\n",
    "                LX-MIE (Kitzmann &amp; Heng 2018b),\n",
    "                celerite2 (Foreman-Mackey et al. 2017; Foreman-Mackey 2018) exoplanet (Foreman-Mackey et al. 2021b),\n",
    "                lightkurve (Lightkurve Collaboration et al. 2018),\n",
    "                corner (Foreman-Mackey 2016),\n",
    "                kelp (Morris et al. 2022).\n",
    "\n",
    "            This research has made use of the SVO Filter Profile Service (http://svo2.cab.inta-csic.es/theory/fps/) supported\n",
    "            from the Spanish MINECO through grant AYA2017-84089.'''\n",
    "    },\n",
    "    {\n",
    "        'title': 'Title 3',\n",
    "        'abstract': 'Climate change is a pressing global issue that requires urgent action to mitigate its impacts on the environment and human societies.',\n",
    "        'release_date': '2024-03-10',\n",
    "        'acknowledgements': '''Acknowledgements. This work is based on data from eROSITA, the soft X-ray instrument aboard SRG,\n",
    "        a joint Russian-German science mission supported by the Russian Space Agency (Roskosmos),\n",
    "        in the interests of the Russian Academy of Sciences represented by its Space Research Institute (IKI),\n",
    "        and the Deutsches Zentrum für Luft-und Raumfahrt (DLR).'''\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make available the packages inside all the modules\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Papers to the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be adding all the papers to the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from article_graph.article_graph import ArticleGraph\n",
    "\n",
    "# We create the graph\n",
    "g = ArticleGraph()\n",
    "\n",
    "# We add the documents to the graph\n",
    "for paper_id, paper_info in enumerate(papers):\n",
    "    g.add_paper(paper_id=paper_id,\n",
    "                title=paper_info['title'],\n",
    "                abstract=paper_info['abstract'],\n",
    "                release_date=paper_info['release_date'])\n",
    "    \n",
    "# Explore the graph by printing the titles of the papers\n",
    "for s, p, o in g.graph.triples((None, g.ns.title, None)):\n",
    "    print(s, p, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be exploring the use of topic modeling inside the **Article Graph** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we will be extracting topics from the papers' abstracts using the `topic_modeling` module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['the', 'is', 'issue', 'on', 'change', 'climate', 'environment']\n",
      "Topic 1: ['and', 'the', 'of', 'behind', 'knowledge', 'legacies', 'civilizations']\n",
      "Topic 2: ['the', 'is', 'of', 'vast', 'planets', 'galaxies', 'countless']\n"
     ]
    }
   ],
   "source": [
    "from topic_modeling.lda import LDA\n",
    "\n",
    "# Create the LDA model for Topic Modeling\n",
    "# We need to specify the number of topics and the number of words per topic\n",
    "lda_model = LDA(corpus=[paper['abstract'] for paper in papers],\n",
    "                num_topics=3,\n",
    "                num_words=7)\n",
    "lda_model.fit()\n",
    "\n",
    "# Display the generated topics\n",
    "for i, topic in enumerate(lda_model.topics):\n",
    "    print(f'Topic {i}: {topic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Topics to Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we will be adding the generated topics to the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://open_science.com/topic#0 http://open_science.com/keyword the\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword the\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword the\n",
      "http://open_science.com/topic#0 http://open_science.com/keyword is\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword is\n",
      "http://open_science.com/topic#0 http://open_science.com/keyword issue\n",
      "http://open_science.com/topic#0 http://open_science.com/keyword on\n",
      "http://open_science.com/topic#0 http://open_science.com/keyword change\n",
      "http://open_science.com/topic#0 http://open_science.com/keyword climate\n",
      "http://open_science.com/topic#0 http://open_science.com/keyword environment\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword and\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword of\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword of\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword behind\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword knowledge\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword legacies\n",
      "http://open_science.com/topic#1 http://open_science.com/keyword civilizations\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword vast\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword planets\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword galaxies\n",
      "http://open_science.com/topic#2 http://open_science.com/keyword countless\n"
     ]
    }
   ],
   "source": [
    "# We add the topics to the graph\n",
    "for topic_id, keywords in enumerate(lda_model.topics):\n",
    "    g.add_topic(topic_id, keywords)\n",
    "\n",
    "# We explore the graph by printing the keywords of each topic\n",
    "for s, p, o in g.graph.triples((None, g.ns.keyword, None)):\n",
    "    print(s, p, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TopicBelongings to Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we will be adding the topic belonging relationships to the graph! These relationships represent the topic dostributions of each paper to every topic in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://open_science.com/paper#0 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#00 http://open_science.com/degree 0.020183839800524628\n",
      "http://open_science.com/paper#0 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#01 http://open_science.com/degree 0.020414235129847587\n",
      "http://open_science.com/paper#0 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#02 http://open_science.com/degree 0.9594019250696278\n",
      "http://open_science.com/paper#1 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#10 http://open_science.com/degree 0.016995018976179124\n",
      "http://open_science.com/paper#1 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#11 http://open_science.com/degree 0.965817619919958\n",
      "http://open_science.com/paper#1 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#12 http://open_science.com/degree 0.017187361103862805\n",
      "http://open_science.com/paper#2 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#20 http://open_science.com/degree 0.967269327731982\n",
      "http://open_science.com/paper#2 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#21 http://open_science.com/degree 0.016351883880151748\n",
      "http://open_science.com/paper#2 http://open_science.com/belongs_to_topic http://open_science.com/topic_belonging#22 http://open_science.com/degree 0.01637878838786634\n"
     ]
    }
   ],
   "source": [
    "# We predict the topic distributions for each paper to all the topics\n",
    "lda_model.predict_all()\n",
    "\n",
    "# We add the topic belonging for each topic and paper storing the degree of belonging\n",
    "for paper_id, paper_info in enumerate(lda_model.topic_distributions):\n",
    "    for topic_id, topic_dist in paper_info.items():\n",
    "        g.add_topic_belonging(paper_id, topic_id, topic_dist)\n",
    "\n",
    "# We explore the graph by printing the topic belonging for each paper to all the topics\n",
    "for s, p, o in g.graph.triples((None, g.ns.belongs_to_topic, None)):\n",
    "    for _, p1, o1 in g.graph.triples((o, g.ns.degree, None)):\n",
    "        print(s, p, o, p1, o1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be exploring the use of named entity recognition inside the **Article Graph** !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be exploring the use of similarity inside the **Article Graph** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we will be calculating the similarity between the papers' abstracts using the `similarity` module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gloria\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity results:\n",
      "{'text_id1': 0, 'text_id2': 1, 'similarity': 0.13612501}\n",
      "{'text_id1': 0, 'text_id2': 2, 'similarity': 0.0900681}\n",
      "{'text_id1': 1, 'text_id2': 2, 'similarity': 0.06453505}\n"
     ]
    }
   ],
   "source": [
    "# pip install -U sentence-transformers\n",
    "from similarity.Model import Model\n",
    "\n",
    "# Name of the SentenceTransformer model to use\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Create an instance of the class\n",
    "Model_instance = Model([paper['abstract'] for paper in papers], model_name)\n",
    "\n",
    "# Calculate similarity and retrieve the results\n",
    "similarity_results = Model_instance.calculate_similarity()\n",
    "\n",
    "# Print similarity results\n",
    "print(\"Similarity results:\")\n",
    "for result in similarity_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding similarity to Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we will be adding the calculated similarity to the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the similarity results and add them to the graph\n",
    "for result in similarity_results:\n",
    "    text_id1 = result['text_id1']\n",
    "    text_id2 = result['text_id2']\n",
    "    similarity_score = result['similarity']\n",
    "    \n",
    "    # Add the similarity to the graph using the add_similarity method\n",
    "    g.add_similarity(text_id1, text_id2, similarity_score)\n",
    "\n",
    "# Explore the graph by printing the similarity between papers\n",
    "for s, p, o in g.graph.triples((None, g.ns.similar_to, None)):\n",
    "    for _, p1, o1 in g.graph.triples((s, g.ns.degree, None)):\n",
    "        print(f\"Paper 1: {s}, Paper 2: {o}, Similarity Score: {o1}\")\n",
    "\n",
    "for s, p, o in g.graph.triples((None, g.ns.similar_from, None)):\n",
    "    for _, p1, o1 in g.graph.triples((s, g.ns.degree, None)):\n",
    "        print(f\"Paper 1: {o1}, Paper 2: {s}, Similarity Score: {o1}\")\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-similarity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
